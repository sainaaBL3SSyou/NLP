{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lab2\n",
        "Тоог нэмэх параллел алгоритм\n",
        "\n"
      ],
      "metadata": {
        "id": "6UsXuZJGwqMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parallel_sum.cpp\n",
        "#include<iostream>\n",
        "#include<omp.h>\n",
        "#include<chrono>\n",
        "using namespace std;\n",
        "\n",
        "#define ARRAY_SIZE 1000000\n",
        "\n",
        "int parallel(int A[]) {\n",
        "    int total_sum = 0;\n",
        "\n",
        "    #pragma omp parallel num_threads(4)\n",
        "    {\n",
        "        int l_sum = 0;\n",
        "        #pragma omp for\n",
        "        for (int i = 0; i < ARRAY_SIZE; ++i) {\n",
        "            l_sum += A[i];\n",
        "        }\n",
        "        //#pragma omp critical\n",
        "        total_sum += l_sum;\n",
        "    }\n",
        "    return total_sum;\n",
        "}\n",
        "\n",
        "int arraySum(int arr[]) {\n",
        "    int sum = 0;\n",
        "    for (int i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        sum += arr[i];\n",
        "    }\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int A[ARRAY_SIZE];\n",
        "\n",
        "    // Populate array A with values\n",
        "    for (int i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        A[i] = i;\n",
        "    }\n",
        "\n",
        "    auto start_time_serial = chrono::high_resolution_clock::now();\n",
        "\n",
        "    int result_serial = arraySum(A);\n",
        "\n",
        "    auto end_time_serial = chrono::high_resolution_clock::now();\n",
        "    chrono::duration<double> serial_duration = end_time_serial - start_time_serial;\n",
        "\n",
        "    auto start_time_parallel = chrono::high_resolution_clock::now();\n",
        "    int parallel_res = parallel(A);\n",
        "    auto end_time_parallel = chrono::high_resolution_clock::now();\n",
        "    chrono::duration<double> parallel_duration = end_time_parallel - start_time_parallel;\n",
        "\n",
        "    cout << \"Total sum of array elements (serial): \" << result_serial << endl;\n",
        "    cout << \"Total sum of array elements (parallel): \" << parallel_res << endl;\n",
        "\n",
        "    cout << \"Serial time: \" << serial_duration.count() << \" seconds\" << endl;\n",
        "    cout << \"Parallel time: \" << parallel_duration.count() << \" seconds\" << endl;\n",
        "    cout << \"Speedup: \" << serial_duration.count() / parallel_duration.count() << endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSw_vLXrwzs6",
        "outputId": "9b820095-f18f-4caa-ddf5-5937a688546b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting parallel_sum.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -o parallel_sum parallel_sum.cpp -fopenmp\n",
        "!./parallel_sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD9Tw1MTBh88",
        "outputId": "9384eade-9649-4eac-db50-d99723c0b6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sum of array elements (serial): 1783293664\n",
            "Total sum of array elements (parallel): 1783293664\n",
            "Serial time: 0.00221937 seconds\n",
            "Parallel time: 0.00179053 seconds\n",
            "Speedup: 1.23951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jacobi iteration алгоритм"
      ],
      "metadata": {
        "id": "WBG-Pj_Yw3jR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "id": "8GNovyfN9tlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKTPs6yujgnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6a4b07-b40f-4d3b-ad70-8c3ffe0b0459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converge, iteration : 144 per process\n",
            "Error : 0.000096\n",
            "Parallel Time: 0.057433128356933594 seconds\n",
            "Serial Time: 2.384185791015625e-07 seconds\n",
            "Speedup: 4.151237899141524e-06\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from mpi4py import MPI\n",
        "\n",
        "# MPI эхлүүлж, MPI коммуникаторын зэрэглэл болон хэмжээг олж авна.\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "MAX_N = 8\n",
        "\n",
        "A = np.zeros((MAX_N-2, MAX_N-2))\n",
        "A = np.pad(A, pad_width=1, mode='constant', constant_values=1)\n",
        "(row_num, col_num) = A.shape\n",
        "B = np.zeros((row_num, col_num))\n",
        "\n",
        "#Процесс бүрт хуваарилагдсан мөрийн тоог тодорхойлно. Нийт мөрийн тоог процессын тоонд хуваахгүй бол сүүлийн процесс нэмэлт мөр авч болно.\n",
        "if rank == (size-1):\n",
        "    local_row_num = int(MAX_N / size) + (MAX_N % size)\n",
        "else:\n",
        "    local_row_num = int(MAX_N / size)\n",
        "\n",
        "#A` болон B матрицуудын хэсгүүдийг зэрэглэлд үндэслэн процесс бүрт хуваарилдаг.\n",
        "local_first_row = rank * int(MAX_N / size)\n",
        "local_last_row = local_first_row + local_row_num\n",
        "A_local = A[local_first_row:local_last_row,]\n",
        "B_local = B[local_first_row:local_last_row,]\n",
        "\n",
        "converge = False\n",
        "iteration_num = 0\n",
        "while not converge:  #Нэгдмэл байдалд хүрэх хүртэл Якоби давталтыг гүйцэтгэнэ.\n",
        "    iteration_num += 1\n",
        "    diffnorm = 0.0\n",
        "\n",
        "    A_local_padding = np.pad(A_local, pad_width=1, mode='constant', constant_values=0)\n",
        "\n",
        "#MPI Send болон Recv үйлдлүүдийг ашиглан хөрш процессуудын хооронд `A_local`-ын  утгуудыг холбодог.\n",
        "    if rank > 0:\n",
        "        comm.Send(A_local[0, ], dest=rank-1, tag=11)\n",
        "        tmp = np.empty(MAX_N, dtype=A_local_padding.dtype)\n",
        "        comm.Recv(tmp, source=rank-1, tag=22)\n",
        "        A_local_padding[0, 1:MAX_N+1] = tmp\n",
        "    if rank < (size - 1):\n",
        "        comm.Send(A_local[local_row_num-1, ], dest=rank+1, tag=22)\n",
        "        tmp = np.empty(MAX_N, dtype=A_local_padding.dtype)\n",
        "        comm.Recv(tmp, source=rank+1, tag=11)\n",
        "        A_local_padding[local_row_num+1, 1:MAX_N+1] = tmp\n",
        "\n",
        "    for i in range(row_num):\n",
        "        for j in range(col_num):\n",
        "            idx_i_A = i + 1\n",
        "            idx_j_A = j + 1\n",
        "            B_local[i][j] = 0.25*(A_local_padding[idx_i_A+1, idx_j_A]\n",
        "                                + A_local_padding[idx_i_A-1, idx_j_A]\n",
        "                                + A_local_padding[idx_i_A, idx_j_A+1]\n",
        "                                + A_local_padding[idx_i_A, idx_j_A-1])\n",
        "            diffnorm += math.sqrt((B_local[i, j] - A_local[i, j])*(B_local[i, j] - A_local[i, j]))\n",
        "    A_local = np.copy(B_local)\n",
        "\n",
        "    diffnorm_glov = comm.allreduce(diffnorm, op=MPI.SUM)\n",
        "# нийлмэл байдлыг шалгана.\n",
        "    if diffnorm_glov <= 0.0001:\n",
        "        print('Converge, iteration : %d per process' % iteration_num)\n",
        "        print('Error : %f' % diffnorm_glov)\n",
        "        converge = True\n",
        "#Бүх процессоос 'A' матрицын локал хэсгүүдийг эх процесс руу (0-р зэрэглэл) цуглуулдаг.\n",
        "comm.Gatherv(A_local, [A, MPI.DOUBLE], root=0)\n",
        "\n",
        "if rank == 0:\n",
        "    end = time.time()\n",
        "    parallel_time = end - start\n",
        "    print('Parallel Time:', parallel_time, 'seconds')\n",
        "\n",
        "if rank == 0:\n",
        "    start = time.time()\n",
        "    end = time.time()\n",
        "    serial_time = end - start\n",
        "    print('Serial Time:', serial_time, 'seconds')\n",
        "\n",
        "if rank == 0:\n",
        "    speedup = serial_time / parallel_time\n",
        "    print('Speedup:', speedup)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from mpi4py import MPI\n",
        "\n",
        "# Function to print the result\n",
        "def print_result(A, rank):\n",
        "    if rank == 0:\n",
        "        print(\"Result Matrix A:\")\n",
        "        print(A)\n",
        "\n",
        "# Main block\n",
        "if __name__ == \"__main__\":\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    MAX_N = 8\n",
        "\n",
        "    A = np.zeros((MAX_N-2, MAX_N-2))\n",
        "    A = np.pad(A, pad_width=1, mode='constant', constant_values=1)\n",
        "    (row_num, col_num) = A.shape\n",
        "    B = np.zeros((row_num, col_num))\n",
        "\n",
        "    if rank == (size-1):\n",
        "        local_row_num = int(MAX_N / size) + (MAX_N % size)\n",
        "    else:\n",
        "        local_row_num = int(MAX_N / size)\n",
        "\n",
        "    local_first_row = rank * int(MAX_N / size)\n",
        "    local_last_row = local_first_row + local_row_num\n",
        "    A_local = A[local_first_row:local_last_row, ]\n",
        "    B_local = B[local_first_row:local_last_row, ]\n",
        "\n",
        "    converge = False\n",
        "    iteration_num = 0\n",
        "    while not converge:\n",
        "        iteration_num += 1\n",
        "        diffnorm = 0.0\n",
        "\n",
        "        A_local_padding = np.pad(A_local, pad_width=1, mode='constant', constant_values=0)\n",
        "\n",
        "        if rank > 0:\n",
        "            comm.Send(A_local[0, ], dest=rank-1, tag=11)\n",
        "            tmp = np.empty(MAX_N, dtype=A_local_padding.dtype)\n",
        "            comm.Recv(tmp, source=rank-1, tag=22)\n",
        "            A_local_padding[0, 1:MAX_N+1] = tmp\n",
        "        if rank < (size - 1):\n",
        "            comm.Send(A_local[local_row_num-1, ], dest=rank+1, tag=22)\n",
        "            tmp = np.empty(MAX_N, dtype=A_local_padding.dtype)\n",
        "            comm.Recv(tmp, source=rank+1, tag=11)\n",
        "            A_local_padding[local_row_num+1, 1:MAX_N+1] = tmp\n",
        "\n",
        "        for i in range(row_num):\n",
        "            for j in range(col_num):\n",
        "                idx_i_A = i + 1\n",
        "                idx_j_A = j + 1\n",
        "                B_local[i][j] = 0.25*(A_local_padding[idx_i_A+1, idx_j_A]\n",
        "                                    + A_local_padding[idx_i_A-1, idx_j_A]\n",
        "                                    + A_local_padding[idx_i_A, idx_j_A+1]\n",
        "                                    + A_local_padding[idx_i_A, idx_j_A-1])\n",
        "                diffnorm += math.sqrt((B_local[i, j] - A_local[i, j])*(B_local[i, j] - A_local[i, j]))\n",
        "        A_local = np.copy(B_local)\n",
        "\n",
        "        diffnorm_glov = comm.allreduce(diffnorm, op=MPI.SUM)\n",
        "        if diffnorm_glov <= 0.0001:\n",
        "            print('Converge, iteration : %d per process' % iteration_num)\n",
        "            print('Error : %f' % diffnorm_glov)\n",
        "            converge = True\n",
        "\n",
        "    comm.Gatherv(A_local, [A, MPI.DOUBLE], root=0)\n",
        "\n",
        "\n",
        "print_result(A, rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKM6DNap-Svt",
        "outputId": "09f16ae8-7786-48f1-af64-311faf6f8dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converge, iteration : 144 per process\n",
            "Error : 0.000096\n",
            "Result Matrix A:\n",
            "[[5.42518598e-06 1.01960145e-05 1.37370531e-05 1.56212004e-05\n",
            "  1.56212004e-05 1.37370531e-05 1.01960145e-05 5.42518598e-06]\n",
            " [1.01960145e-05 1.91622391e-05 2.58172149e-05 2.93582536e-05\n",
            "  2.93582536e-05 2.58172149e-05 1.91622391e-05 1.01960145e-05]\n",
            " [1.37370531e-05 2.58172149e-05 3.47834396e-05 3.95542680e-05\n",
            "  3.95542680e-05 3.47834396e-05 2.58172149e-05 1.37370531e-05]\n",
            " [1.56212004e-05 2.93582536e-05 3.95542680e-05 4.49794540e-05\n",
            "  4.49794540e-05 3.95542680e-05 2.93582536e-05 1.56212004e-05]\n",
            " [1.56212004e-05 2.93582536e-05 3.95542680e-05 4.49794540e-05\n",
            "  4.49794540e-05 3.95542680e-05 2.93582536e-05 1.56212004e-05]\n",
            " [1.37370531e-05 2.58172149e-05 3.47834396e-05 3.95542680e-05\n",
            "  3.95542680e-05 3.47834396e-05 2.58172149e-05 1.37370531e-05]\n",
            " [1.01960145e-05 1.91622391e-05 2.58172149e-05 2.93582536e-05\n",
            "  2.93582536e-05 2.58172149e-05 1.91622391e-05 1.01960145e-05]\n",
            " [5.42518598e-06 1.01960145e-05 1.37370531e-05 1.56212004e-05\n",
            "  1.56212004e-05 1.37370531e-05 1.01960145e-05 5.42518598e-06]]\n"
          ]
        }
      ]
    }
  ]
}